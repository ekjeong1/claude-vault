# 엔트로피

## 개념

엔트로피(Entropy)는 데이터의 불확실성 또는 무질서도를 측정하는 정보 이론의 핵심 개념이다.

## 정의

Shannon 엔트로피:
```
H(X) = -Σ p(x) log₂ p(x)
```

- 확률 분포가 균등할수록 엔트로피가 높다
- 확실성이 높을수록 엔트로피가 낮다

## 의사결정에서의 활용

**정보 가치 평가**
- 정보 획득으로 인한 엔트로피 감소 = 정보 가치
- 의사결정 트리에서 최적 분기 선택

**신호 대 잡음 비율**
- 높은 엔트로피 = 높은 잡음
- 낮은 엔트로피 = 명확한 신호

**불확실성 정량화**
- 예측 모델의 신뢰도 평가
- 리스크 수준 측정

## 응용 분야

- 머신러닝: 의사결정 트리, 특성 선택
- 통신: 데이터 압축, 채널 용량
- 투자: 포트폴리오 다양화 평가

## 관련 개념

- [[최적화 이론]] - 엔트로피를 활용한 의사결정 트리의 최적 분기 선택
- [[동적 계획법과 강화학습]] - 불확실한 환경에서의 적응적 의사결정
- [[극값 이론]] - 확률 분포와 불확실성 정량화의 또 다른 측면
