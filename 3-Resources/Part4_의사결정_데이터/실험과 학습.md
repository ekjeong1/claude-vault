# 실험과 학습: A/B 테스트와 탐색-활용

**학습하면서 최적화하기**

**↑ [[0_Part4_Decision_Index|Part 4 인덱스로 돌아가기]]**

---

## 개요

비즈니스 의사결정에서 가장 강력한 도구는 **실험(Experimentation)**이다. 어떤 전략이 최선인지 확신할 수 없을 때, 작은 규모로 여러 옵션을 테스트하면서 동시에 최적의 선택을 찾아가는 것이 핵심이다.

### 핵심 질문

- **어떤 옵션이 최선인가?** → A/B 테스트
- **여러 옵션을 동시에 테스트하면서 손실을 최소화하려면?** → Multi-Armed Bandit
- **새로운 것을 시도할 것인가, 아는 것을 활용할 것인가?** → 탐색-활용 트레이드오프
- **복잡한 파라미터 공간에서 최적값을 찾으려면?** → 베이지안 최적화

---

## 1. A/B 테스트: 통제된 실험

### 핵심 개념

**A/B 테스트**는 두 개 이상의 대안을 무작위로 배정하여 어느 것이 더 나은지 통계적으로 검증하는 방법이다.

```
사용자 모집단
    ↓
무작위 배정
    ├─→ A 그룹 (기존 버전)  → 전환율 측정
    └─→ B 그룹 (새 버전)    → 전환율 측정
         ↓
    통계적 유의성 검정
         ↓
    의사결정: A vs B
```

### 실무 프로세스

#### 1단계: 가설 설정
```
귀무가설 (H0): A와 B의 전환율은 같다
대립가설 (H1): B의 전환율이 A보다 높다
```

#### 2단계: 표본 크기 결정
```
필요 표본 크기 n ≈ 16 × σ² / δ²

여기서:
- σ: 전환율의 표준편차
- δ: 탐지하고 싶은 최소 차이 (Minimum Detectable Effect)
- 유의수준 α = 0.05, 검정력 1-β = 0.8 가정
```

#### 3단계: 실험 실행
- 무작위 배정 (Randomization)
- 충분한 기간 운영 (최소 1주일 권장)
- 외부 요인 통제 (요일 효과, 계절성 등)

#### 4단계: 통계적 검정
```
t-검정 또는 z-검정:
t = (p_B - p_A) / √(p(1-p) × (1/n_A + 1/n_B))

p-value < 0.05 이면 통계적으로 유의
```

### 실무 사례

**배달의민족 - 메인 화면 UI 개선**
- **A 버전**: 기존 가로 스크롤 배너
- **B 버전**: 세로 카드 형식 배너
- **측정 지표**: 클릭률(CTR), 주문 전환율
- **결과**: B 버전의 CTR 15% 증가, 전환율 8% 증가 → B 버전 채택

---

## 2. Multi-Armed Bandit: 탐색하면서 활용하기

### 핵심 개념

**Multi-Armed Bandit (MAB)**은 여러 슬롯머신(팔) 중 어느 것이 가장 높은 보상을 주는지 모르는 상황에서, **실험과 수익을 동시에 최적화**하는 알고리즘이다.

A/B 테스트와의 차이:
- **A/B 테스트**: 실험 기간 동안 50:50 고정 배분 → 실험 후 결정
- **MAB**: 실시간으로 성과 좋은 옵션에 더 많은 트래픽 배정 → 기회비용 최소화

### 주요 알고리즘

#### ε-Greedy (엡실론 탐욕)
```python
# 확률 ε로 탐색, (1-ε)로 활용
if random() < ε:
    선택 = 무작위_팔()  # 탐색 (Exploration)
else:
    선택 = 최고_평균_보상_팔()  # 활용 (Exploitation)
```

**장점**: 단순하고 직관적
**단점**: ε이 고정되어 있어 시간에 따라 비효율적

#### UCB (Upper Confidence Bound)
```
각 팔의 점수 = 평균_보상 + √(2 × ln(총_시도) / 팔_시도)
                 ↑ 활용        ↑ 탐색 보너스

점수가 가장 높은 팔 선택
```

**핵심**: 불확실성이 높은 팔(시도 적은 팔)에 보너스를 줘서 탐색 유도
**장점**: 이론적으로 최적에 가까운 성능 (Regret 최소화)

#### Thompson Sampling (톰슨 샘플링)
```
각 팔의 보상 분포를 베이지안 방식으로 업데이트
매 라운드마다:
  1. 각 팔의 사후 분포에서 샘플링
  2. 가장 높은 값을 가진 팔 선택
  3. 결과로 사후 분포 업데이트
```

**장점**: UCB보다 실무에서 더 나은 성능
**실무 적용**: 구글, 넷플릭스 등에서 사용

### 실무 사례

**넷플릭스 - 썸네일 최적화**
- **문제**: 수백만 작품마다 여러 썸네일 중 어느 것이 클릭을 유도하는가?
- **방법**: Thompson Sampling으로 각 사용자 세그먼트에 최적 썸네일 학습
- **결과**: 클릭률 30% 증가, 동시에 실험 기회비용 최소화

---

## 3. 탐색-활용 트레이드오프 (Exploration-Exploitation Tradeoff)

### 핵심 딜레마

```
탐색 (Exploration)              활용 (Exploitation)
새로운 옵션 시도                알려진 최선의 옵션 사용
정보 획득                       즉각적 수익
장기적 최적화                   단기적 최적화
불확실성 감소                   확실한 보상
```

**핵심 질문**: 언제까지 탐색하고, 언제부터 활용할 것인가?

### 수학적 프레임워크

#### Regret 최소화
```
Regret = Σ(최적_팔_보상 - 선택한_팔_보상)

목표: 누적 Regret 최소화
```

#### 정보 가치 (Value of Information)
```
VoI = E[실험 후 이익] - E[실험 없이 이익]

실험 비용 < VoI  →  탐색
실험 비용 > VoI  →  활용
```

### 실무 적용

#### 초기 단계 (정보 부족)
- **전략**: 적극적 탐색
- **예**: 신제품 출시 초기 → 다양한 마케팅 채널 테스트

#### 성장 단계 (어느 정도 정보 확보)
- **전략**: 균형잡힌 탐색-활용
- **예**: MAB로 지속적 학습하면서 수익 최적화

#### 성숙 단계 (충분한 정보)
- **전략**: 주로 활용, 간헐적 탐색
- **예**: 시장 변화 감지 위해 10% 트래픽으로 새 전략 테스트

### 실무 사례

**페이스북 뉴스피드 알고리즘**
- **탐색**: 사용자가 클릭한 적 없는 새 콘텐츠 소량 노출 (10%)
- **활용**: 과거 반응 좋았던 콘텐츠 주로 노출 (90%)
- **적응**: 사용자 행동 변화 시 탐색 비율 동적 조정

---

## 4. 베이지안 최적화 (Bayesian Optimization)

### 핵심 개념

**베이지안 최적화**는 비싼(시간/비용이 많이 드는) 함수의 최댓값을 찾을 때, **최소한의 시도로 최적값을 찾는** 방법이다.

**적용 상황**:
- 하이퍼파라미터 튜닝 (ML 모델)
- A/B 테스트가 너무 비싼 경우
- 실험 하나에 오래 걸리는 경우 (예: 제조업 공정 최적화)

### 작동 원리

```
1. 대리 모델 (Surrogate Model) 구축
   → Gaussian Process로 함수 근사

2. 획득 함수 (Acquisition Function) 최대화
   → 어디를 다음에 시도할지 결정

3. 실제 평가 → 대리 모델 업데이트

4. 반복
```

#### Acquisition Function

**Expected Improvement (EI)**
```
EI(x) = E[max(f(x) - f(x_best), 0)]

의미: x를 시도했을 때 현재 최선보다 얼마나 개선될 것으로 예상되는가?
```

**특징**:
- 불확실성 높은 영역 → 탐색
- 높은 값 예상되는 영역 → 활용
- 자동으로 탐색-활용 균형

### 실무 프로세스

```python
# 의사코드
while 예산 남음:
    # 1. GP로 현재까지 데이터 학습
    gp_model.fit(X_observed, y_observed)

    # 2. EI 최대화하는 다음 지점 찾기
    x_next = maximize(expected_improvement)

    # 3. 실제 실험
    y_next = expensive_experiment(x_next)

    # 4. 데이터 추가
    X_observed.append(x_next)
    y_observed.append(y_next)
```

### 실무 사례

**구글 - 광고 입찰 전략 최적화**
- **문제**: 수십 개 파라미터(입찰 가격, 타겟팅, 시간대 등)의 최적 조합
- **기존 방법**: Grid Search → 수백만 조합 필요
- **베이지안 최적화**: 50-100번 시도로 최적 조합 발견
- **결과**: 실험 시간 90% 단축, ROI 25% 개선

---

## 통합: 4가지 방법 언제 사용할까?

| 상황 | 방법 | 이유 |
|------|------|------|
| 2개 옵션 비교, 충분한 트래픽 | A/B 테스트 | 명확한 통계적 검증 |
| 여러 옵션, 실시간 최적화 | Multi-Armed Bandit | 기회비용 최소화 |
| 장기 전략, 환경 변화 | 탐색-활용 프레임워크 | 지속적 학습 |
| 비싼 실험, 복잡한 파라미터 | 베이지안 최적화 | 시도 횟수 최소화 |

---

## 관련 개념

### BrainTwin 프레임워크 내 연결

- **[[인과 추론]]**: A/B 테스트는 인과 추론의 가장 강력한 도구 (RCT)
- **[[엔트로피]]**: 탐색은 불확실성(엔트로피)을 줄이는 과정
- **[[베이즈 정리]]** (Part 2): 베이지안 최적화와 Thompson Sampling의 기반
- **[[최적화 이론]]**: 활용 단계는 최적화 문제 해결
- **[[동적 계획법과 강화학습]]**: MAB는 강화학습의 가장 단순한 형태

### 다른 Part와 연결

- **Part 2 [[베이즈 정리]]**: 사후 확률 업데이트 메커니즘 공유
- **Part 3 [[Phase Transition]]**: 탐색→활용 전환은 일종의 상전이
- **Part 5 [[TRIZ와 Isomorphism]]**: 실험 설계는 문제 해결의 체계적 접근

---

## 실무 체크리스트

### A/B 테스트 실행 전
- [ ] 명확한 가설과 측정 지표 정의
- [ ] 필요 표본 크기 계산
- [ ] 무작위 배정 메커니즘 검증
- [ ] 외부 요인(계절성, 요일 효과) 고려
- [ ] 테스트 기간 충분히 설정 (최소 1주)

### Multi-Armed Bandit 도입 시
- [ ] 실시간 의사결정 가능한 시스템인가?
- [ ] 트래픽이 충분한가? (일 1000+ 이상 권장)
- [ ] 빠른 피드백 루프가 있는가?
- [ ] Thompson Sampling vs UCB 선택 (보통 Thompson 권장)
- [ ] 초기 탐색 기간 설정 (Burn-in period)

### 탐색-활용 균형 설정
- [ ] 현재 불확실성 수준 평가
- [ ] 실험 비용 vs 정보 가치 계산
- [ ] 시장 변화 속도 고려
- [ ] 탐색 비율 정기 검토 (월 1회)

### 베이지안 최적화 적용
- [ ] 실험 비용이 충분히 높은가? (시간/금전)
- [ ] 파라미터 범위 사전 정의
- [ ] 대리 모델 선택 (Gaussian Process 기본)
- [ ] 초기 랜덤 샘플 수집 (5-10개)
- [ ] 수렴 기준 설정

---

## 비즈니스 케이스 스터디

### 케이스 1: 쿠팡 - 추천 알고리즘 최적화

**상황**:
- 수백 개 추천 알고리즘 후보
- 각 알고리즘 평가에 1주일 필요
- 시즌 세일 전까지 최적 알고리즘 선택 필요

**적용 방법**: 베이지안 최적화
1. 주요 하이퍼파라미터 5개 식별
2. 초기 10개 랜덤 조합 테스트 (10주)
3. 베이지안 최적화로 다음 20개 조합 선택 (20주)
4. 30번 시도로 최적 조합 발견

**결과**:
- 기존 Grid Search: 1년 이상 소요 예상
- 베이지안 최적화: 30주 만에 완료
- 클릭률 18% 증가, 구매 전환율 12% 증가

---

### 케이스 2: 토스 - 홈 화면 위젯 순서 최적화

**상황**:
- 10개 위젯의 최적 배치 순서 찾기
- 사용자 세그먼트별로 다른 순서가 최적일 수 있음
- 실시간으로 최적화 필요

**적용 방법**: Contextual Multi-Armed Bandit
1. 사용자 컨텍스트 (연령, 주 사용 기능, 시간대) 반영
2. Thompson Sampling으로 각 세그먼트 최적 순서 학습
3. 2주마다 성과 리뷰 및 탐색 비율 조정

**결과**:
- 개인화된 홈 화면으로 체류 시간 25% 증가
- 핵심 기능 사용률 40% 증가
- A/B 테스트 대비 기회비용 70% 절감

---

### 케이스 3: 무신사 - 프로모션 타이밍 최적화

**상황**:
- 주중/주말, 시간대별 최적 프로모션 타이밍 불명확
- 시즌마다 패턴 변화
- 과도한 프로모션은 마진 악화

**적용 방법**: 탐색-활용 프레임워크
1. **초기 (2주)**: 적극적 탐색 - 모든 시간대 골고루 테스트
2. **학습 (4주)**: ε-greedy (ε=0.2) - 80%는 최적 타이밍, 20%는 탐색
3. **활용 (시즌 내내)**: ε=0.05 - 주로 최적 타이밍 활용, 간헐적 탐색

**결과**:
- 프로모션 효율성 35% 증가 (같은 비용으로 매출 증가)
- 시즌 변화 조기 감지 (여름→가을 전환)
- 마진율 유지하면서 매출 증대

---

## 함정과 주의사항

### 1. **조기 종료의 함정**
❌ **문제**: A/B 테스트를 통계적 유의성 나오자마자 종료
✅ **해결**: 사전에 표본 크기와 기간을 정하고 준수

### 2. **다중 비교 문제 (Multiple Testing)**
❌ **문제**: 10개 지표 중 하나가 우연히 유의하게 나올 수 있음
✅ **해결**: 본페로니 보정 또는 주요 지표 사전 정의

### 3. **신규성 효과 (Novelty Effect)**
❌ **문제**: 새로운 UI가 처음엔 좋아 보이지만 시간이 지나면 효과 감소
✅ **해결**: 최소 2주 이상 테스트, 시간별 트렌드 분석

### 4. **과도한 탐색**
❌ **문제**: 계속 새로운 것만 시도하면 최적값 찾아도 활용 못 함
✅ **해결**: 명확한 탐색→활용 전환 기준 설정

### 5. **컨텍스트 무시**
❌ **문제**: 모든 사용자에게 같은 최적값 적용
✅ **해결**: Contextual Bandit으로 세그먼트별 최적화

---

## 핵심 요약

### 3가지 핵심 원칙

1. **실험 없이 확신 없다**
   - 직관보다 데이터
   - 작게 시작, 빠르게 학습

2. **탐색은 투자, 활용은 수확**
   - 초기: 적극적 탐색
   - 성숙: 간헐적 탐색 + 주로 활용

3. **실시간 최적화**
   - 고정된 전략보다 적응적 전략
   - MAB로 학습하면서 수익 최적화

### 의사결정 트리

```
실험 필요?
 ├─ 예 → 옵션이 2개?
 │       ├─ 예 → A/B 테스트
 │       └─ 아니오 → 실시간 최적화 필요?
 │                   ├─ 예 → Multi-Armed Bandit
 │                   └─ 아니오 → 파라미터 복잡?
 │                               ├─ 예 → 베이지안 최적화
 │                               └─ 아니오 → A/B 테스트
 └─ 아니오 → 탐색-활용 균형 체크
```

---

#resource #braintwin #part4 #experimentation #ab-testing #multi-armed-bandit #bayesian-optimization #exploration-exploitation
